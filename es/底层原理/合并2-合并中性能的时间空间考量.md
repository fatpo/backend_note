<!--ts-->
   * [写在前面](#写在前面)
   * [空间-合并策略](#空间-合并策略)
      * [调整空间策略的经验1：适当提高限速](#调整空间策略的经验1适当提高限速)
      * [调整空间策略的经验2：放弃不关键字段](#调整空间策略的经验2放弃不关键字段)
   * [时间-间隔策略](#时间-间隔策略)
   * [手动合并要警惕！](#手动合并要警惕)
   * [原文搬运](#原文搬运)

<!-- Created by https://github.com/ekalinin/github-markdown-toc -->
<!-- Added by: jianguo.ouyang, at: Mon Jan 30 18:58:30 CST 2023 -->

<!--te-->

## 写在前面
按常理来说，合并多个 segments，多个小的合并成为一个大的。

一定有一堆的配置来限制，比如控制在 1000 个以下？比如控制大小 100MB 以内？总之一定有一些边界限制的。

## 空间-合并策略

合并线程是按照一定的运行策略来挑选 segment 进行归并的。主要有以下几条：

* index.merge.policy.floor_segment：默认 2MB，小于该值的 segment 会优先被归并。
* index.merge.policy.max_merge_at_once：默认一次最多归并 10 个 segment
* index.merge.policy.max_merge_at_once_explicit：默认 forcemerge 时一次最多归并 30 个 segment
* index.merge.policy.max_merged_segment：默认 5 GB，大于该值的 segment，不用参与归并，forcemerge 除外


查看 es 的策略：
```text
 curl -XGET 'http:///10.1.96.5:9200/2023-01-30/_settings?include_defaults&pretty'

"policy" : {
    "floor_segment" : "2mb",
    "max_merge_at_once_explicit" : "30",
    "max_merge_at_once" : "10",
    "max_merged_segment" : "5gb",
    "expunge_deletes_allowed" : "10.0",
    "segments_per_tier" : "10.0",
    "deletes_pct_allowed" : "33.0"
}
          
解释下：    
"policy": {
    "fool_segment": "2mb",
    "max_merge_at_once_explicit": "30", //显示调用optimize 操作或者 expungeDeletes时可以操作多少个segments，默认是30
    "max_merge_at_once": "10",//一次最多只操作多少个segments，默认是10.
    "max_merged_segment": "5gb",//超过多大size的segment不会再做merge，默认是5g
    "expunge_deletes_allowed": "10.0",//指删除了的文档数在一个segment里占的百分比，默认是10，大于这个值时，在执行expungeDeletes 操作时将会merge这些segments.
    "segments_per_tier": "10.0"//每个tier允许的segment 数，注意这个数要大于上面的at_once数，否则这个值会先于最大可操作数到达，就会立刻做merge，这样会造成频繁的访问操作
}
```

### 调整空间策略的经验1：适当提高限速

默认情况下，归并线程的限速配置 indices.store.throttle.max_bytes_per_sec 是 20MB。

对于写入量较大，磁盘转速较高，甚至使用 SSD 盘的服务器来说，这个限速是明显过低的。

对于 ELK Stack 应用，建议可以适当调大到 100MB或者更高。设置方式如下：
```text
PUT /_cluster/settings
{
    "persistent" : {
        "indices.store.throttle.max_bytes_per_sec" : "100mb"
    }
}
```

或者不限制：
```text
PUT /_cluster/settings
{
    "transient" : {
        "indices.store.throttle.type" : "none"
    }
}
```

### 调整空间策略的经验2：放弃不关键字段
norms、doc_values 和 stored 字段的存储机制类似，每个 field 有一个全量的存储，对存储浪费很大。

* norms: 如果一个 field 不需要考虑其相关度分数，那么可以禁用 norms，减少倒排索引内存占用量，字段粒度配置 omit_norms=true；  norms记录了索引中index-time boost信息，但是当你进行搜索时可能会比较耗费内存。omit_norms = true则是忽略掉域加权信息，这样在搜索的时候就不会处理索引时刻的加权信息了。
* doc_values: 如果不需要对 field 进行排序或者聚合，那么可以禁用 doc_values 字段；
* store: 如果 field 只需要提供搜索，不需要返回则将 stored 设为 false； 域存储选项store，用来确定是否需要存储域的真实值，以便后续搜集时能恢复这个值。

这样子从根本上减少 segment 是的体积，也是一个提速的小妙招。




## 时间-间隔策略

上面的`合并策略`是讲合并的时候要干嘛，这个属于空间范畴。

还有一个时间范畴，就是 加大 refresh 间隔，尽量让每次新生成的 segment 本身大小就比较大。

这种方式主要通过延迟提交实现，延迟提交意味着数据从提交到搜索可见有延迟，具体需要结合业务配置，默认值1s；

针对索引节点粒度的配置如下：
```
curl -XPUT http://host地址:port端口/索引节点名称/_settings -d '{"index.refresh_interval":"10s"}'
```

## 手动合并要警惕！
手动合并 _forcemerge 的 API 如下：
```text
//该命令针对5.0以上版本的ES
POST  /INDEX/_forcemerge
{
  "max_num_segments":1
}

//5.0以下可以使用以下
POST  /INDEX/_optimize
{
  "max_num_segments":1
}
```

它有很坑的点就是：
```text
_forcemerge 命令是没有限制资源的，也就是你系统有多少IO资源就会使用多少IO资源，
这样可能导致一段时间内搜索没有任何响应!
```

它非常适合那些，静态索引，比如昨天的日志，上个月的数据报表，等几乎不会改动的。

如果非要改动，请务必把索引文件先挪到一个 node 上，目标是不要影响别人。

但具体怎么做呢？有没有一些方法论？

## 原文搬运
* [Elasticsearch搜索引擎：ES的segment段合并原理](https://blog.csdn.net/a745233700/article/details/117953198)
* [Elasticsearch 理解mapping中的store属性](https://www.cnblogs.com/hahaha111122222/p/12157453.html)